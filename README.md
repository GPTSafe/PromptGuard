![PromptSafe](https://i.imgur.com/0VKLJ37.png)

<div align="center">
  <a href="https://github.com/GPTSafe/PromptSafe/blob/main/CONTRIBUTING.md"><img src="https://img.shields.io/badge/PRs-welcome-brightgreen.svg" /></a>
  <a href="https://github.com/GPTSafe/PromptSafe/blob/main/LICENSE"><img src="https://img.shields.io/badge/license-Apache%202-blue" /></a>
    <a href="https://github.com/GPTSafe/PromptSafe/blob/main/LICENSE"><img src="https://img.shields.io/npm/v/promptsafe" /></a>
  <br />
  <br />
</div>

## What is PromptSafe?
PromptSafe prevents GPT prompt attacks in Node.js and TypeScript backend applications.

PromptSafe is still in a very early state. You can stay updated with new releases by clicking **watch** -> **custom** -> **releases** in the top right corner.

## Project Goals
The goal of the PromptSafe project is to provide comprehensive sanitization and validation for potentially unsafe GPT prompts and inputs. 

The following is a non-exhaustive list of proposed features:
* Deny lists
* Allow lists
* Content filtering
* Input length limiting
* Input normalization
* Input obfuscation
* Output encoding